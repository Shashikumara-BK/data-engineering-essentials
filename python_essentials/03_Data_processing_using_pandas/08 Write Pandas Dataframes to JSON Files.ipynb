{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "986da23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_column_names(schemas, ds_name, sorting_key='column_position'):\n",
    "    column_details = schemas[ds_name]\n",
    "    columns = sorted(column_details, key=lambda col: col[sorting_key])\n",
    "    return [col['column_name'] for col in columns]\n",
    "\n",
    "\n",
    "schemas = json.load(open('data/retail_db/schemas.json'))\n",
    "\n",
    "orders_columns = get_column_names(schemas, 'orders')\n",
    "\n",
    "orders = pd.read_csv('data/retail_db/orders/part-00000',names=orders_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d51eac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "orders.to_csv(\n",
      "    path_or_buf: \u001b[33m'FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    *,\n",
      "    sep: \u001b[33m'str'\u001b[39m = \u001b[33m','\u001b[39m,\n",
      "    na_rep: \u001b[33m'str'\u001b[39m = \u001b[33m''\u001b[39m,\n",
      "    float_format: \u001b[33m'str | Callable | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    columns: \u001b[33m'Sequence[Hashable] | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    header: \u001b[33m'bool_t | list[str]'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    index: \u001b[33m'bool_t'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    index_label: \u001b[33m'IndexLabel | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    mode: \u001b[33m'str'\u001b[39m = \u001b[33m'w'\u001b[39m,\n",
      "    encoding: \u001b[33m'str | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    compression: \u001b[33m'CompressionOptions'\u001b[39m = \u001b[33m'infer'\u001b[39m,\n",
      "    quoting: \u001b[33m'int | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    quotechar: \u001b[33m'str'\u001b[39m = \u001b[33m'\"'\u001b[39m,\n",
      "    lineterminator: \u001b[33m'str | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    chunksize: \u001b[33m'int | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    date_format: \u001b[33m'str | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    doublequote: \u001b[33m'bool_t'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    escapechar: \u001b[33m'str | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    decimal: \u001b[33m'str'\u001b[39m = \u001b[33m'.'\u001b[39m,\n",
      "    errors: \u001b[33m'OpenFileErrors'\u001b[39m = \u001b[33m'strict'\u001b[39m,\n",
      "    storage_options: \u001b[33m'StorageOptions | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ") -> \u001b[33m'str | None'\u001b[39m\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Write object to a comma-separated values (csv) file.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "path_or_buf : str, path object, file-like object, or None, default None\n",
      "    String, path object (implementing os.PathLike[str]), or file-like\n",
      "    object implementing a write() function. If None, the result is\n",
      "    returned as a string. If a non-binary file object is passed, it should\n",
      "    be opened with `newline=''`, disabling universal newlines. If a binary\n",
      "    file object is passed, `mode` might need to contain a `'b'`.\n",
      "sep : str, default ','\n",
      "    String of length 1. Field delimiter for the output file.\n",
      "na_rep : str, default ''\n",
      "    Missing data representation.\n",
      "float_format : str, Callable, default None\n",
      "    Format string for floating point numbers. If a Callable is given, it takes\n",
      "    precedence over other numeric formatting parameters, like decimal.\n",
      "columns : sequence, optional\n",
      "    Columns to write.\n",
      "header : bool or list of str, default True\n",
      "    Write out the column names. If a list of strings is given it is\n",
      "    assumed to be aliases for the column names.\n",
      "index : bool, default True\n",
      "    Write row names (index).\n",
      "index_label : str or sequence, or False, default None\n",
      "    Column label for index column(s) if desired. If None is given, and\n",
      "    `header` and `index` are True, then the index names are used. A\n",
      "    sequence should be given if the object uses MultiIndex. If\n",
      "    False do not print fields for index names. Use index_label=False\n",
      "    for easier importing in R.\n",
      "mode : {'w', 'x', 'a'}, default 'w'\n",
      "    Forwarded to either `open(mode=)` or `fsspec.open(mode=)` to control\n",
      "    the file opening. Typical values include:\n",
      "\n",
      "    - 'w', truncate the file first.\n",
      "    - 'x', exclusive creation, failing if the file already exists.\n",
      "    - 'a', append to the end of file if it exists.\n",
      "\n",
      "encoding : str, optional\n",
      "    A string representing the encoding to use in the output file,\n",
      "    defaults to 'utf-8'. `encoding` is not supported if `path_or_buf`\n",
      "    is a non-binary file object.\n",
      "compression : str or dict, default 'infer'\n",
      "    For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' is\n",
      "    path-like, then detect compression from the following extensions: '.gz',\n",
      "    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      "    (otherwise no compression).\n",
      "    Set to ``None`` for no compression.\n",
      "    Can also be a dict with key ``'method'`` set\n",
      "    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
      "    other key-value pairs are forwarded to\n",
      "    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "    ``bz2.BZ2File``, ``zstandard.ZstdCompressor``, ``lzma.LZMAFile`` or\n",
      "    ``tarfile.TarFile``, respectively.\n",
      "    As an example, the following could be passed for faster compression and to create\n",
      "    a reproducible gzip archive:\n",
      "    ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n",
      "\n",
      "    .. versionadded:: 1.5.0\n",
      "        Added support for `.tar` files.\n",
      "\n",
      "       May be a dict with key 'method' as compression mode\n",
      "       and other entries as additional compression options if\n",
      "       compression mode is 'zip'.\n",
      "\n",
      "       Passing compression options as keys in dict is\n",
      "       supported for compression modes 'gzip', 'bz2', 'zstd', and 'zip'.\n",
      "quoting : optional constant from csv module\n",
      "    Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n",
      "    then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n",
      "    will treat them as non-numeric.\n",
      "quotechar : str, default '\\\"'\n",
      "    String of length 1. Character used to quote fields.\n",
      "lineterminator : str, optional\n",
      "    The newline character or character sequence to use in the output\n",
      "    file. Defaults to `os.linesep`, which depends on the OS in which\n",
      "    this method is called ('\\\\n' for linux, '\\\\r\\\\n' for Windows, i.e.).\n",
      "\n",
      "    .. versionchanged:: 1.5.0\n",
      "\n",
      "        Previously was line_terminator, changed for consistency with\n",
      "        read_csv and the standard library 'csv' module.\n",
      "\n",
      "chunksize : int or None\n",
      "    Rows to write at a time.\n",
      "date_format : str, default None\n",
      "    Format string for datetime objects.\n",
      "doublequote : bool, default True\n",
      "    Control quoting of `quotechar` inside a field.\n",
      "escapechar : str, default None\n",
      "    String of length 1. Character used to escape `sep` and `quotechar`\n",
      "    when appropriate.\n",
      "decimal : str, default '.'\n",
      "    Character recognized as decimal separator. E.g. use ',' for\n",
      "    European data.\n",
      "errors : str, default 'strict'\n",
      "    Specifies how encoding and decoding errors are to be handled.\n",
      "    See the errors argument for :func:`open` for a full list\n",
      "    of options.\n",
      "\n",
      "storage_options : dict, optional\n",
      "    Extra options that make sense for a particular storage connection, e.g.\n",
      "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "    are forwarded to ``urllib.request.Request`` as header options. For other\n",
      "    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      "    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      "    details, and for more examples on storage options refer `here\n",
      "    <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      "    highlight=storage_options#reading-writing-remote-files>`_.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "None or str\n",
      "    If path_or_buf is None, returns the resulting csv format as a\n",
      "    string. Otherwise returns None.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "read_csv : Load a CSV file into a DataFrame.\n",
      "to_excel : Write DataFrame to an Excel file.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "Create 'out.csv' containing 'df' without indices\n",
      "\n",
      ">>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n",
      "...                    'mask': ['red', 'purple'],\n",
      "...                    'weapon': ['sai', 'bo staff']})\n",
      ">>> df.to_csv('out.csv', index=False)  # doctest: +SKIP\n",
      "\n",
      "Create 'out.zip' containing 'out.csv'\n",
      "\n",
      ">>> df.to_csv(index=False)\n",
      "'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n",
      ">>> compression_opts = dict(method='zip',\n",
      "...                         archive_name='out.csv')  # doctest: +SKIP\n",
      ">>> df.to_csv('out.zip', index=False,\n",
      "...           compression=compression_opts)  # doctest: +SKIP\n",
      "\n",
      "To write a csv file to a new folder or nested folder you will first\n",
      "need to create it using either Pathlib or os:\n",
      "\n",
      ">>> from pathlib import Path  # doctest: +SKIP\n",
      ">>> filepath = Path('folder/subfolder/out.csv')  # doctest: +SKIP\n",
      ">>> filepath.parent.mkdir(parents=True, exist_ok=True)  # doctest: +SKIP\n",
      ">>> df.to_csv(filepath)  # doctest: +SKIP\n",
      "\n",
      ">>> import os  # doctest: +SKIP\n",
      ">>> os.makedirs('folder/subfolder', exist_ok=True)  # doctest: +SKIP\n",
      ">>> df.to_csv('folder/subfolder/out.csv')  # doctest: +SKIP\n",
      "\u001b[31mFile:\u001b[39m      d:\\devops_projects\\python\\python-for-de\\pr-venv\\lib\\site-packages\\pandas\\core\\generic.py\n",
      "\u001b[31mType:\u001b[39m      method"
     ]
    }
   ],
   "source": [
    "orders.to_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cced299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.makedirs('data/retail_db/orders_json', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b841dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "orders.to_json(\n",
      "    path_or_buf: \u001b[33m'FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    *,\n",
      "    orient: \u001b[33m\"Literal['split', 'records', 'index', 'table', 'columns', 'values'] | None\"\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    date_format: \u001b[33m'str | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    double_precision: \u001b[33m'int'\u001b[39m = \u001b[32m10\u001b[39m,\n",
      "    force_ascii: \u001b[33m'bool_t'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    date_unit: \u001b[33m'TimeUnit'\u001b[39m = \u001b[33m'ms'\u001b[39m,\n",
      "    default_handler: \u001b[33m'Callable[[Any], JSONSerializable] | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    lines: \u001b[33m'bool_t'\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    compression: \u001b[33m'CompressionOptions'\u001b[39m = \u001b[33m'infer'\u001b[39m,\n",
      "    index: \u001b[33m'bool_t | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    indent: \u001b[33m'int | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    storage_options: \u001b[33m'StorageOptions | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    mode: \u001b[33m\"Literal['a', 'w']\"\u001b[39m = \u001b[33m'w'\u001b[39m,\n",
      ") -> \u001b[33m'str | None'\u001b[39m\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Convert the object to a JSON string.\n",
      "\n",
      "Note NaN's and None will be converted to null and datetime objects\n",
      "will be converted to UNIX timestamps.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "path_or_buf : str, path object, file-like object, or None, default None\n",
      "    String, path object (implementing os.PathLike[str]), or file-like\n",
      "    object implementing a write() function. If None, the result is\n",
      "    returned as a string.\n",
      "orient : str\n",
      "    Indication of expected JSON string format.\n",
      "\n",
      "    * Series:\n",
      "\n",
      "        - default is 'index'\n",
      "        - allowed values are: {'split', 'records', 'index', 'table'}.\n",
      "\n",
      "    * DataFrame:\n",
      "\n",
      "        - default is 'columns'\n",
      "        - allowed values are: {'split', 'records', 'index', 'columns',\n",
      "          'values', 'table'}.\n",
      "\n",
      "    * The format of the JSON string:\n",
      "\n",
      "        - 'split' : dict like {'index' -> [index], 'columns' -> [columns],\n",
      "          'data' -> [values]}\n",
      "        - 'records' : list like [{column -> value}, ... , {column -> value}]\n",
      "        - 'index' : dict like {index -> {column -> value}}\n",
      "        - 'columns' : dict like {column -> {index -> value}}\n",
      "        - 'values' : just the values array\n",
      "        - 'table' : dict like {'schema': {schema}, 'data': {data}}\n",
      "\n",
      "        Describing the data, where data component is like ``orient='records'``.\n",
      "\n",
      "date_format : {None, 'epoch', 'iso'}\n",
      "    Type of date conversion. 'epoch' = epoch milliseconds,\n",
      "    'iso' = ISO8601. The default depends on the `orient`. For\n",
      "    ``orient='table'``, the default is 'iso'. For all other orients,\n",
      "    the default is 'epoch'.\n",
      "double_precision : int, default 10\n",
      "    The number of decimal places to use when encoding\n",
      "    floating point values. The possible maximal value is 15.\n",
      "    Passing double_precision greater than 15 will raise a ValueError.\n",
      "force_ascii : bool, default True\n",
      "    Force encoded string to be ASCII.\n",
      "date_unit : str, default 'ms' (milliseconds)\n",
      "    The time unit to encode to, governs timestamp and ISO8601\n",
      "    precision.  One of 's', 'ms', 'us', 'ns' for second, millisecond,\n",
      "    microsecond, and nanosecond respectively.\n",
      "default_handler : callable, default None\n",
      "    Handler to call if object cannot otherwise be converted to a\n",
      "    suitable format for JSON. Should receive a single argument which is\n",
      "    the object to convert and return a serialisable object.\n",
      "lines : bool, default False\n",
      "    If 'orient' is 'records' write out line-delimited json format. Will\n",
      "    throw ValueError if incorrect 'orient' since others are not\n",
      "    list-like.\n",
      "compression : str or dict, default 'infer'\n",
      "    For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' is\n",
      "    path-like, then detect compression from the following extensions: '.gz',\n",
      "    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      "    (otherwise no compression).\n",
      "    Set to ``None`` for no compression.\n",
      "    Can also be a dict with key ``'method'`` set\n",
      "    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
      "    other key-value pairs are forwarded to\n",
      "    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "    ``bz2.BZ2File``, ``zstandard.ZstdCompressor``, ``lzma.LZMAFile`` or\n",
      "    ``tarfile.TarFile``, respectively.\n",
      "    As an example, the following could be passed for faster compression and to create\n",
      "    a reproducible gzip archive:\n",
      "    ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n",
      "\n",
      "    .. versionadded:: 1.5.0\n",
      "        Added support for `.tar` files.\n",
      "\n",
      "    .. versionchanged:: 1.4.0 Zstandard support.\n",
      "\n",
      "index : bool or None, default None\n",
      "    The index is only used when 'orient' is 'split', 'index', 'column',\n",
      "    or 'table'. Of these, 'index' and 'column' do not support\n",
      "    `index=False`.\n",
      "\n",
      "indent : int, optional\n",
      "   Length of whitespace used to indent each record.\n",
      "\n",
      "storage_options : dict, optional\n",
      "    Extra options that make sense for a particular storage connection, e.g.\n",
      "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "    are forwarded to ``urllib.request.Request`` as header options. For other\n",
      "    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      "    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      "    details, and for more examples on storage options refer `here\n",
      "    <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      "    highlight=storage_options#reading-writing-remote-files>`_.\n",
      "\n",
      "mode : str, default 'w' (writing)\n",
      "    Specify the IO mode for output when supplying a path_or_buf.\n",
      "    Accepted args are 'w' (writing) and 'a' (append) only.\n",
      "    mode='a' is only supported when lines is True and orient is 'records'.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "None or str\n",
      "    If path_or_buf is None, returns the resulting json format as a\n",
      "    string. Otherwise returns None.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "read_json : Convert a JSON string to pandas object.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The behavior of ``indent=0`` varies from the stdlib, which does not\n",
      "indent the output but does insert newlines. Currently, ``indent=0``\n",
      "and the default ``indent=None`` are equivalent in pandas, though this\n",
      "may change in a future release.\n",
      "\n",
      "``orient='table'`` contains a 'pandas_version' field under 'schema'.\n",
      "This stores the version of `pandas` used in the latest revision of the\n",
      "schema.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from json import loads, dumps\n",
      ">>> df = pd.DataFrame(\n",
      "...     [[\"a\", \"b\"], [\"c\", \"d\"]],\n",
      "...     index=[\"row 1\", \"row 2\"],\n",
      "...     columns=[\"col 1\", \"col 2\"],\n",
      "... )\n",
      "\n",
      ">>> result = df.to_json(orient=\"split\")\n",
      ">>> parsed = loads(result)\n",
      ">>> dumps(parsed, indent=4)  # doctest: +SKIP\n",
      "{\n",
      "    \"columns\": [\n",
      "        \"col 1\",\n",
      "        \"col 2\"\n",
      "    ],\n",
      "    \"index\": [\n",
      "        \"row 1\",\n",
      "        \"row 2\"\n",
      "    ],\n",
      "    \"data\": [\n",
      "        [\n",
      "            \"a\",\n",
      "            \"b\"\n",
      "        ],\n",
      "        [\n",
      "            \"c\",\n",
      "            \"d\"\n",
      "        ]\n",
      "    ]\n",
      "}\n",
      "\n",
      "Encoding/decoding a Dataframe using ``'records'`` formatted JSON.\n",
      "Note that index labels are not preserved with this encoding.\n",
      "\n",
      ">>> result = df.to_json(orient=\"records\")\n",
      ">>> parsed = loads(result)\n",
      ">>> dumps(parsed, indent=4)  # doctest: +SKIP\n",
      "[\n",
      "    {\n",
      "        \"col 1\": \"a\",\n",
      "        \"col 2\": \"b\"\n",
      "    },\n",
      "    {\n",
      "        \"col 1\": \"c\",\n",
      "        \"col 2\": \"d\"\n",
      "    }\n",
      "]\n",
      "\n",
      "Encoding/decoding a Dataframe using ``'index'`` formatted JSON:\n",
      "\n",
      ">>> result = df.to_json(orient=\"index\")\n",
      ">>> parsed = loads(result)\n",
      ">>> dumps(parsed, indent=4)  # doctest: +SKIP\n",
      "{\n",
      "    \"row 1\": {\n",
      "        \"col 1\": \"a\",\n",
      "        \"col 2\": \"b\"\n",
      "    },\n",
      "    \"row 2\": {\n",
      "        \"col 1\": \"c\",\n",
      "        \"col 2\": \"d\"\n",
      "    }\n",
      "}\n",
      "\n",
      "Encoding/decoding a Dataframe using ``'columns'`` formatted JSON:\n",
      "\n",
      ">>> result = df.to_json(orient=\"columns\")\n",
      ">>> parsed = loads(result)\n",
      ">>> dumps(parsed, indent=4)  # doctest: +SKIP\n",
      "{\n",
      "    \"col 1\": {\n",
      "        \"row 1\": \"a\",\n",
      "        \"row 2\": \"c\"\n",
      "    },\n",
      "    \"col 2\": {\n",
      "        \"row 1\": \"b\",\n",
      "        \"row 2\": \"d\"\n",
      "    }\n",
      "}\n",
      "\n",
      "Encoding/decoding a Dataframe using ``'values'`` formatted JSON:\n",
      "\n",
      ">>> result = df.to_json(orient=\"values\")\n",
      ">>> parsed = loads(result)\n",
      ">>> dumps(parsed, indent=4)  # doctest: +SKIP\n",
      "[\n",
      "    [\n",
      "        \"a\",\n",
      "        \"b\"\n",
      "    ],\n",
      "    [\n",
      "        \"c\",\n",
      "        \"d\"\n",
      "    ]\n",
      "]\n",
      "\n",
      "Encoding with Table Schema:\n",
      "\n",
      ">>> result = df.to_json(orient=\"table\")\n",
      ">>> parsed = loads(result)\n",
      ">>> dumps(parsed, indent=4)  # doctest: +SKIP\n",
      "{\n",
      "    \"schema\": {\n",
      "        \"fields\": [\n",
      "            {\n",
      "                \"name\": \"index\",\n",
      "                \"type\": \"string\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"col 1\",\n",
      "                \"type\": \"string\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"col 2\",\n",
      "                \"type\": \"string\"\n",
      "            }\n",
      "        ],\n",
      "        \"primaryKey\": [\n",
      "            \"index\"\n",
      "        ],\n",
      "        \"pandas_version\": \"1.4.0\"\n",
      "    },\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"index\": \"row 1\",\n",
      "            \"col 1\": \"a\",\n",
      "            \"col 2\": \"b\"\n",
      "        },\n",
      "        {\n",
      "            \"index\": \"row 2\",\n",
      "            \"col 1\": \"c\",\n",
      "            \"col 2\": \"d\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\u001b[31mFile:\u001b[39m      d:\\devops_projects\\python\\python-for-de\\pr-venv\\lib\\site-packages\\pandas\\core\\generic.py\n",
      "\u001b[31mType:\u001b[39m      method"
     ]
    }
   ],
   "source": [
    "orders.to_json?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fffb9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = orders.to_json('data/retail_db/orders_json/part-00000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee2799f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "670d64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data using row format\n",
    "b = orders.to_json('data/retail_db/orders_json/part-00000', orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "550d2011",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ba70642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>order_customer_id</th>\n",
       "      <th>order_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>11599</td>\n",
       "      <td>CLOSED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>256</td>\n",
       "      <td>PENDING_PAYMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>12111</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>8827</td>\n",
       "      <td>CLOSED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>11318</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68878</th>\n",
       "      <td>68879</td>\n",
       "      <td>2014-07-09 00:00:00.0</td>\n",
       "      <td>778</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68879</th>\n",
       "      <td>68880</td>\n",
       "      <td>2014-07-13 00:00:00.0</td>\n",
       "      <td>1117</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68880</th>\n",
       "      <td>68881</td>\n",
       "      <td>2014-07-19 00:00:00.0</td>\n",
       "      <td>2518</td>\n",
       "      <td>PENDING_PAYMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68881</th>\n",
       "      <td>68882</td>\n",
       "      <td>2014-07-22 00:00:00.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>ON_HOLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68882</th>\n",
       "      <td>68883</td>\n",
       "      <td>2014-07-23 00:00:00.0</td>\n",
       "      <td>5533</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68883 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       order_id             order_date  order_customer_id     order_status\n",
       "0             1  2013-07-25 00:00:00.0              11599           CLOSED\n",
       "1             2  2013-07-25 00:00:00.0                256  PENDING_PAYMENT\n",
       "2             3  2013-07-25 00:00:00.0              12111         COMPLETE\n",
       "3             4  2013-07-25 00:00:00.0               8827           CLOSED\n",
       "4             5  2013-07-25 00:00:00.0              11318         COMPLETE\n",
       "...         ...                    ...                ...              ...\n",
       "68878     68879  2014-07-09 00:00:00.0                778         COMPLETE\n",
       "68879     68880  2014-07-13 00:00:00.0               1117         COMPLETE\n",
       "68880     68881  2014-07-19 00:00:00.0               2518  PENDING_PAYMENT\n",
       "68881     68882  2014-07-22 00:00:00.0              10000          ON_HOLD\n",
       "68882     68883  2014-07-23 00:00:00.0               5533         COMPLETE\n",
       "\n",
       "[68883 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json('data/retail_db/orders_json/part-00000',lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pr-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
